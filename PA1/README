user: <jakerach> 
user: <cponcede>

WRITEUP FOR ASSIGNMENT 1:

Design Decisions:

The design for most of our lexer was pretty straightforward. We created regular
expressions for each identifier and keyword. The difficulty came in dealing with
strings and comments. To simplify strings and comments, we used start
conditions.

For comments, we had a single start condition called comment. One line comments
did not need this start condition, and from now on we will assume that comments
refers to the the multi-line form. When the lexer recognizes a "(*" token in the
normal start condition, it moves into the comment start condition and increment
our comment_depth counter. Now, every time in the comment start condition we see
a (* we increment our comment_depth, and every time we see a *) we decrement. If
we see a *) and our comment_depth drops to zero, we know we are at the end of a
comment, and we can leave our comment start condition. All other characters in
the comment are ignored with a . rule. With this approach, it is easy to
identify an extra *) (if we see it while in normal condition, it doesn't have a
corresponding (* ), as well as an unmatched (* (if we see EOF while in a
comment, we know that we never matched a parenthesis).

Now that we have taken care of comments, we must deal with strings. Here the
special cases are more varied. We have two start conditions: string and
invalid_string. When we see a quotation mark in normal mode, we switch to the
string mode, and when we see another quotation in string mode, we switch back.
While in string mode, we replace all escaped characters with the actual
character that the input string was attempting to represent. We also look
for invalid characters in a string (\0, EOF, etc) with regex, handle them
accordingly, and switch into invalid_string mode if any invalid character
is found or if the string becomes too long. This mode just ignores all
characters until we see another quotation so we don't have to deal with all the
string special cases in a string that is already error filled.

Testing:

We test our lexer with three files we created on top of the provided tests:
comments_test.cl, keyword_test.cl, and string_tests.cl.

comments_test.cl is a file that contains multiple different comments. We test
all of our comment start condition regex cases. This includes mismatched comment
opening and closing, escaped new lines, EOF bugs, and lots of nested commenting.

string_tests.cl is a file that contains multiple different strings. We test both
start conditions that have to do with strings. We do this by purposefully
creating both valid and invalid strings. The special cases include escaped
characters, escaped new lines, and strings that are too long.

keyword_test.cl tests the rest of our keywords and identifiers. We don't
actually need to write cool code to test our lexer, since we just want to make
sure that keywords are properly picked up by our regex. In this file, we test
case insensitivity and valid recognition of all one/two character operators.


Completeness:

We know that our lexer is complete since, for every possible start condition,
regular expression that will  match all tokens not handled by
the more specific rules. Thus, it is impossible for a token to be unmatched.

