user: <jakerach> user: <cponcede>

WRITEUP FOR ASSIGNMENT 1:

Design Decisions:

The design for most of our lexer was pretty straightforward. We created regular
expressions for each identifier and keyword. The difficulty came in dealing with
strings and comments. To simplify strings and comments, we used start
conditions.

For comments, we had a single start condition called comment. One line comments
did not need this start condition, and from now on we will assume that comments
refers to the the multi-line form. When the lexer recognizes a "(*" token in the
normal start condition, it moves into the comment start condition and increment
our comment_depth counter. Now, every time in the comment start condition we see
a (* we increment our comment_depth, and every time we see a *) we decrement. If
we see a *) and our comment_depth drops to zero, we know we are at the end of a
comment, and we can leave our comment start condition. All other characters in
the comment are ignored with a . rule. With this approach, it is easy to
identify an extra *) (if we see it while in normal condition, it doesn't have a
corresponding (* ), as well as an unmatched (* (if we see EOF while in a
comment, we know that we never matched a parenthesis).

Now that we have taken care of comments, we must deal with strings. Here the
special cases are more varied. We have two start conditions: string and
invalid_string. When we see a quotation mark in normal mode, we switch to the
string mode, and when we see another quotation in string mode, we switch back.
While in string mode, we replace all escaped characters with the actual
character that the input string was attempting to represent. We also look
for invalid characters in a string (\0, EOF, etc) with regex, handle them
accordingly, and switch into invalid_string mode if any invalid character
is found or if the string becomes too long. This mode just ignores all
characters until we see another quotation so we don't have to deal with all the
string special cases in a string that is already error filled.

Testing:

The primary testing for the lexer is performed by attempting to tokenize
random_tests.cl. This file is not a valid COOL program but thoroughly
tests each of the different types of tokens that the lexical analyzer is
expected to recognize. In addition, the file tests nested comments,
invalid comments, and strings that are invalid for a variety of reasons.
On top of the testing done within random_tests.cl, more specific tests
regarding special cases within strings, such as escaped newlines, are
performed by tokenizing string_tests.cl.

In order to efficiently catch potential bugs in our lexer, we talked about
test cases with Jack Payette while creating random_tests.cl 

Completeness:

We know that our lexer is complete since, for every possible start condition,
regular expression that will  match all tokens not handled by
the more specific rules. Thus, it is impossible for a token to be unmatched.

